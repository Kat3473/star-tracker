{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1mLzXbs-Nap8MtVLL-ylc38MVgtp3RssS",
      "authorship_tag": "ABX9TyNwiYuCplKaA5sFxicKj4Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kat3473/star-tracker/blob/main/Polaris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Here is the following step-by-step workflow to train and implement the AI model onto the Arduino.\n",
        "\n",
        "\n",
        "1.   Setup the Python environment on Colab\n",
        "2.   Collect and upload the data\n",
        "3.   Process the data through Colab\n",
        "4.   Train & test the AI model on Colab\n",
        "5.   Convert the model into TensorFlow Lite\n",
        "6.   Encode the model as an Arduino header file and upload it onto the board\n",
        "\n"
      ],
      "metadata": {
        "id": "ww4w0adfQr2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Imports"
      ],
      "metadata": {
        "id": "DLQlBg-5D86Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all neccessary libraries onto the drive:\n",
        "\n",
        "\n",
        "*   Standard Python Libraries\n",
        "*   TensorFlow and Keras\n",
        "*   Libraries for image processing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sixf6xFnEY0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xFsZ27xDopK"
      },
      "outputs": [],
      "source": [
        "#Import: General\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Import: Image Processing\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "#Import: ML model\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Construction"
      ],
      "metadata": {
        "id": "lmZ1IXFIEbkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images to train and test the model are grabbed directly from google drive to construct the correct datasets.\n",
        "\n",
        "For this particular case, an image dataset of images of the night sky are being used. It is assumed that the brighest and only visible star in the night sky is Polaris."
      ],
      "metadata": {
        "id": "vJ4q37B6H3DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining dataset parameters\n",
        "batch_size = 32 #Size of batches for the data\n",
        "img_height = 224 #Height of the images\n",
        "img_width = 224 #Width of the images\n",
        "\n",
        "#Grabbing the images\n",
        "dataset_url = \"/content/drive/MyDrive/Star Tracker/Dataset\" #google drive of the images\n",
        "data_dir = pathlib.Path(dataset_url).with_suffix('') #sets up the directory to be used\n",
        "classnames = ['polaris'] #Array to assign a label to the class\n",
        "\n",
        "#Training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir, #the directory being converted\n",
        "  validation_split=0.2, #20% used for validation, 80% for actual training\n",
        "  subset=\"training\", #this is the training subset being outputted\n",
        "  seed=132, #seed to randomise data order and seperation\n",
        "  image_size=(img_height, img_width), #converts image sizes to required\n",
        "  batch_size=batch_size, #the batch size of the dataset\n",
        "  class_names=classnames, #assinging labels to the classes\n",
        "  label_mode='int' #controls the datatype of the class labels i.e. [1, 0]\n",
        "  )\n",
        "\n",
        "#Validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory( #same as above but for the validation dataset\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\", #for validation subset\n",
        "  seed=132,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  class_names=classnames,\n",
        "  label_mode='int'\n",
        "  )\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (images, labels))\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (images, labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "yOhUfTt4Ek50",
        "outputId": "3fc8709f-db08-4ca6-9b57-25dd25dc35bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 61 files belonging to 1 classes.\n",
            "Using 49 files for training.\n",
            "Found 61 files belonging to 1 classes.\n",
            "Using 12 files for validation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_ds = train_ds.map(lambda images, labels:\\n                        (images, labels))\\nval_ds = val_ds.map(lambda images, labels:\\n                    (images, labels))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Pre-trained\n",
        "\n",
        "We'll be using transfer learning by building a much smaller 'model' onto the MobileNet neural network."
      ],
      "metadata": {
        "id": "zZrcI6PLJMxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = keras.applications.MobileNetV3Small(\n",
        "    input_shape=(img_height,img_width,3),\n",
        "    alpha=1.0,\n",
        "    minimalistic=False,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    classes=1000,\n",
        "    pooling=None,\n",
        "    dropout_rate=0.2,\n",
        "    classifier_activation=\"softmax\",\n",
        "    include_preprocessing=True,\n",
        "    name=\"MobileNetV3Small\",\n",
        ")\n",
        "\n",
        "num_classes = 1\n",
        "x = model.output\n",
        "x = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "model = keras.models.Model(inputs=model.input, outputs=x)"
      ],
      "metadata": {
        "id": "vtr3q5njJNE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "oD1CPN8OOS2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model\n",
        "model.compile(\n",
        "  optimizer='adamax', #using adamax optimizer\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(), #using sparce categorical crossentropy loss function\n",
        "  metrics=[keras.metrics.SparseCategoricalAccuracy() #using accuracy (multi-class model version) and poisson metrics\n",
        "          ]\n",
        ")\n",
        "\n",
        "#Training the model\n",
        "score = model.fit(\n",
        "  train_ds, #training dataset\n",
        "  validation_data=val_ds, #validation dataset\n",
        "  epochs=1, #number of training steps i.e. number of times the model is trained using the training dataset\n",
        ").history #Recording the metrics into the array 'score'"
      ],
      "metadata": {
        "id": "6iFO-bmnOSc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Loss per epoch\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "line1 = plt.plot(score[\"loss\"], label='Training')\n",
        "line2 = plt.plot(score[\"val_loss\"], label='Validation')\n",
        "plt.legend()\n",
        "\n",
        "#Plotting Accuracy per epoch\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(score[\"sparse_categorical_accuracy\"], label='Training')\n",
        "plt.plot(score[\"val_sparse_categorical_accuracy\"], label='Validation')\n",
        "plt.legend()\n",
        "\n",
        "print(score)"
      ],
      "metadata": {
        "id": "Gd0bOSQWSKE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "PeoSiouCSZ3h"
      }
    }
  ]
}