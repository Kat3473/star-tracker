{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1mLzXbs-Nap8MtVLL-ylc38MVgtp3RssS",
      "authorship_tag": "ABX9TyNE6DkszEQUZ7IqhhypyutG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kat3473/star-tracker/blob/main/Polaris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Here is the following step-by-step workflow to train and implement the AI model onto the Arduino.\n",
        "\n",
        "\n",
        "1.   Setup the Python environment on Colab\n",
        "2.   Collect and upload the data\n",
        "3.   Process the data through Colab\n",
        "4.   Train & test the AI model on Colab\n",
        "5.   Convert the model into TensorFlow Lite\n",
        "6.   Encode the model as an Arduino header file and upload it onto the board\n",
        "\n"
      ],
      "metadata": {
        "id": "ww4w0adfQr2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Imports"
      ],
      "metadata": {
        "id": "DLQlBg-5D86Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the mediapipe library onto the Colab session. This will allow us to run object detection tasks directly on here. Perfrect for testing the model."
      ],
      "metadata": {
        "id": "9Ls8_VpKjc9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe\n",
        "!apt-get -qq install xxd"
      ],
      "metadata": {
        "id": "o4lWVZ0UhqQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b2782a-0de7-4519-a67f-fb86bfb4580e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite_support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqLP3tVR1KjM",
        "outputId": "bb9d529a-0d6b-4dbd-a74e-e1e22d55f71b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tflite_support in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (1.26.4)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (25.2.10)\n",
            "Requirement already satisfied: protobuf<4,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (3.20.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (0.5.1)\n",
            "Requirement already satisfied: pybind11>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (2.13.6)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->tflite_support) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite_support) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all neccessary libraries onto the drive:\n",
        "\n",
        "\n",
        "*   Standard Python Libraries\n",
        "*   TensorFlow and Keras\n",
        "*   Libraries for image processing\n",
        "*   Object Detection\n",
        "*   Converting to TFLite & writing metadata\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sixf6xFnEY0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1xFsZ27xDopK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "66e884b0-3ad7-4e37-e5a2-c2c521ef01d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'xxd'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b8e927557891>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxxd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Import: Image Processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xxd'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#Import: General\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import xxd\n",
        "\n",
        "#Import: Image Processing\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "#Import: ML model\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "#Import: Object Detection\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "#Import: TFLite & Metadata writing\n",
        "from tflite_support.metadata_writers import object_detector\n",
        "from tflite_support.metadata_writers import writer_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Construction"
      ],
      "metadata": {
        "id": "lmZ1IXFIEbkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images to train and test the model are grabbed directly from google drive to construct the correct datasets.\n",
        "\n",
        "For this particular case, an image dataset of images of the night sky are being used. It is assumed that the brighest and only visible star in the night sky is Polaris."
      ],
      "metadata": {
        "id": "vJ4q37B6H3DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining dataset parameters\n",
        "batch_size = 32 #Size of batches for the data\n",
        "img_height = 224 #Height of the images\n",
        "img_width = 224 #Width of the images\n",
        "\n",
        "#Grabbing the images\n",
        "dataset_url = \"/content/drive/MyDrive/Star Tracker/Dataset\" #google drive of the images\n",
        "data_dir = pathlib.Path(dataset_url).with_suffix('') #sets up the directory to be used\n",
        "classnames = ['polaris'] #Array to assign a label to the class\n",
        "\n",
        "#Training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir, #the directory being converted\n",
        "  validation_split=0.2, #20% used for validation, 80% for actual training\n",
        "  subset=\"training\", #this is the training subset being outputted\n",
        "  seed=132, #seed to randomise data order and seperation\n",
        "  image_size=(img_height, img_width), #converts image sizes to required\n",
        "  batch_size=batch_size, #the batch size of the dataset\n",
        "  class_names=classnames, #assinging labels to the classes\n",
        "  label_mode='int' #controls the datatype of the class labels i.e. [1, 0]\n",
        "  )\n",
        "\n",
        "#Validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory( #same as above but for the validation dataset\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\", #for validation subset\n",
        "  seed=132,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  class_names=classnames,\n",
        "  label_mode='int'\n",
        "  )\n",
        "\n",
        "#Dataset pre-processing\n",
        "preprocess_input = keras.applications.mobilenet_v3.preprocess_input #data pre-processing function for ResNet-50\n",
        "\n",
        "#Running the echocardiogram images through a pre-processing neural network layer and saving the new results\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocess_input(images), labels))\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (preprocess_input(images), labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOhUfTt4Ek50",
        "outputId": "71e302a2-822a-47f1-8008-8a47dd4a2d36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 61 files belonging to 1 classes.\n",
            "Using 49 files for training.\n",
            "Found 61 files belonging to 1 classes.\n",
            "Using 12 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Pre-trained\n",
        "\n",
        "We'll be using transfer learning by building a much smaller 'model' onto the MobileNet neural network."
      ],
      "metadata": {
        "id": "zZrcI6PLJMxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = keras.applications.MobileNetV3Small(\n",
        "    input_shape=(img_height, img_width, 3),  # Define input shape\n",
        "    alpha=1.0,\n",
        "    minimalistic=False,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    classes=1000,\n",
        "    pooling=None,\n",
        "    dropout_rate=0.2,\n",
        "    classifier_activation=\"softmax\",\n",
        "    include_preprocessing=False,\n",
        "    name=\"MobileNetV3Small\",\n",
        ")\n",
        "\n",
        "num_classes = 1\n",
        "x = model.output\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dense(1, activation='sigmoid', name='bounding_box')(x)\n",
        "model = keras.models.Model(inputs=model.input, outputs=x)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vtr3q5njJNE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "oD1CPN8OOS2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model\n",
        "model.compile(\n",
        "  optimizer='adam', #using adam optimizer\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(), #using sparce categorical crossentropy loss function\n",
        "  metrics=[keras.metrics.SparseCategoricalAccuracy() #using accuracy (multi-class model version) and poisson metrics\n",
        "          ]\n",
        ")\n",
        "\n",
        "#Training the model\n",
        "score = model.fit(\n",
        "  train_ds, #training dataset\n",
        "  validation_data=val_ds, #validation dataset\n",
        "  epochs=3, #number of training steps i.e. number of times the model is trained using the training dataset\n",
        ").history #Recording the metrics into the array 'score'\n",
        "\n",
        "mp"
      ],
      "metadata": {
        "id": "6iFO-bmnOSc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "adb44f82-43bf-4bba-c14d-9063606c2b7e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  /content/drive/MyDrive/Star Tracker/Dataset/polaris/IMG_20200807_215334533_BURST011.jpg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) NOT_FOUND:  /content/drive/MyDrive/Star Tracker/Dataset/polaris/IMG_20200807_215334533_BURST011.jpg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_173426]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-b618c4530ee0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m score = model.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  /content/drive/MyDrive/Star Tracker/Dataset/polaris/IMG_20200807_215334533_BURST011.jpg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) NOT_FOUND:  /content/drive/MyDrive/Star Tracker/Dataset/polaris/IMG_20200807_215334533_BURST011.jpg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_173426]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Loss per epoch\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylim([0,1])\n",
        "line1 = plt.plot(score[\"loss\"], label='Training')\n",
        "line2 = plt.plot(score[\"val_loss\"], label='Validation')\n",
        "plt.legend()\n",
        "\n",
        "#Plotting Accuracy per epoch\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(score[\"sparse_categorical_accuracy\"], label='Training')\n",
        "plt.plot(score[\"val_sparse_categorical_accuracy\"], label='Validation')\n",
        "plt.legend()\n",
        "\n",
        "print(score)"
      ],
      "metadata": {
        "id": "Gd0bOSQWSKE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting"
      ],
      "metadata": {
        "id": "lK2FAv33kLPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will save, convert and export the trained model as a TFLite model file to be used. The exported model will be imported in the object detection section for testing and fine-tuning."
      ],
      "metadata": {
        "id": "PacERh_4kNGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir = \"/content/drive/MyDrive/Star Tracker/Model/Polaris/\"\n",
        "model.export(saved_model_dir + 'Polaris.h5')"
      ],
      "metadata": {
        "id": "xxEngDJzofVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/drive/MyDrive/Star Tracker/Model/Polaris/Polaris.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "kbNtlRcekM0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/drive/MyDrive/StarTracker/Model/Polaris/Polaris.h\n",
        "!cat /content/drive/MyDrive/StarTracker/Model/Polaris/Polaris.tflite | xxd -i      >> /content/drive/MyDrive/StarTracker/Model/Polaris/Polaris.h\n",
        "!echo \"};\"                              >> /content/drive/MyDrive/StarTracker/Model/Polaris/Polaris.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize('/content/drive/MyDrive/StarTracker/Model/Polaris/Polaris.h')\n",
        "print(f\"Header file is {model_h_size:,} bytes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OcH4d4KSl0g",
        "outputId": "0cc21d60-536f-4e2a-8527-21a94f682559"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header file is 23,029,970 bytes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing Metadata"
      ],
      "metadata": {
        "id": "xCkq5GSgzkAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has been converted to TFLite, we need to write the metadata for it before it's useable for object detection testing."
      ],
      "metadata": {
        "id": "-mPTYD34zoWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_MODEL_PATH = \"/content/drive/MyDrive/Star Tracker/Model/Polaris/Polaris.tflite\"\n",
        "_LABEL_FILE = \"/content/drive/MyDrive/Star Tracker/Model/Polaris/labels.txt\"\n",
        "_SAVE_TO_PATH = \"/content/drive/MyDrive/Star Tracker/Model/Polaris/Polaris_metadata.tflite\""
      ],
      "metadata": {
        "id": "SrThro5t0Urk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ObjectDetectorWriter = object_detector.MetadataWriter\n",
        "\n",
        "_INPUT_NORM_MEAN = 127.5\n",
        "_INPUT_NORM_STD = 127.5\n",
        "\n",
        "# Create the metadata writer.\n",
        "writer = ObjectDetectorWriter.create_for_inference(\n",
        "    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],\n",
        "    [_LABEL_FILE])\n",
        "\n",
        "# Verify the metadata generated by metadata writer.\n",
        "print(writer.get_metadata_json())\n",
        "\n",
        "# Populate the metadata into the model.\n",
        "writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)"
      ],
      "metadata": {
        "id": "Iisz-5-6zlLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection"
      ],
      "metadata": {
        "id": "dIVpQUSghkoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualize function will allow us to visualize the results from the object detection test scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dR5Yz28jTXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MARGIN = 10  # pixels\n",
        "ROW_SIZE = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "TEXT_COLOR = (255, 0, 0)  # red\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image,\n",
        "    detection_result\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  for detection in detection_result.detections:\n",
        "    # Draw bounding_box\n",
        "    bbox = detection.bounding_box\n",
        "    start_point = bbox.origin_x, bbox.origin_y\n",
        "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "    cv2.rectangle(image, start_point, end_point, TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    category_name = category.category_name\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = category_name + ' (' + str(probability) + ')'\n",
        "    text_location = (MARGIN + bbox.origin_x,\n",
        "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
        "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "uZSMuY7hitgb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will implement the object detection features.\n",
        "\n",
        "Note: This is still in progress and is currently using an externally imported TFLite model for testing."
      ],
      "metadata": {
        "id": "dhD8tKkHj05j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_options = python.BaseOptions(model_asset_path='/content/drive/MyDrive/Star Tracker/Model/Polaris/Polaris_metadata.tflite')\n",
        "options = vision.ObjectDetectorOptions(base_options=base_options,\n",
        "                                       score_threshold=0.5)\n",
        "detector = vision.ObjectDetector.create_from_options(options)\n",
        "\n",
        "image = mp.Image.create_from_file('/content/drive/MyDrive/Star Tracker/RS1695_AL4I4805_ad78662bc5c0b4a1f0fa41c3e05dc51c.jpg')\n",
        "\n",
        "detection_result = detector.detect(image)\n",
        "\n",
        "image_copy = np.copy(image.numpy_view())\n",
        "annotated_image = visualize(image_copy, detection_result)\n",
        "rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(rgb_annotated_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3Db9rCozhvFO",
        "outputId": "d26c7c9a-dd39-4d2e-9672-0c36d013fdec"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected a model with 2 or 4 output tensors, found 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-9bb7caac1c7d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m options = vision.ObjectDetectorOptions(base_options=base_options,\n\u001b[1;32m      3\u001b[0m                                        score_threshold=0.5)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectDetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Star Tracker/RS1695_AL4I4805_ad78662bc5c0b4a1f0fa41c3e05dc51c.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/tasks/python/vision/object_detector.py\u001b[0m in \u001b[0;36mcreate_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mtask_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[0;32m--> 238\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    239\u001b[0m         task_info.generate_graph_config(\n\u001b[1;32m    240\u001b[0m             \u001b[0menable_flow_limiting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0;34m'callback should not be provided.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       )\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TaskRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacket_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected a model with 2 or 4 output tensors, found 1."
          ]
        }
      ]
    }
  ]
}